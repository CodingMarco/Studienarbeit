\subsection{Laden und vorbereiten des Datensatzes}
\label{sec:DatensatzLaden}
Um den Datensatz als Trainingsdaten für das neuronale Netz zu verwenden, muss er in ein numpy-Array geladen werden.
Zunächst ist das Ziel, die Frames des gesamten Datensatzes in ein numpy-Array mit der Form $(x,~50,~50)$ zu bringen, wobei $x$ der Anzahl an Frames entspricht.
Dafür kann der Code aus \autoref{lst:GdalOpen} verwendet werden, wobei über jedes Datum vom 17.07.2014 bis 25.10.2021 iteriert wird.
Somit ergibt sich ein Array der Form $(2652,~50,~50)$.
Die einzelnen Pixel enthalten bisher noch die summierte Standdauer der Radarkontrollen.
Mit dem Wissen um die gewichtete Verlustfunktion ist jedoch klar geworden, dass es sich bei der Problemstellung eigentlich um eine Binärklassifizierung handelt.
Daher muss nun ein Grenzwert ausgewählt werden, ab dem eine Rasterzelle als "`positiv"' gilt.
Auch hier muss zwischen der Verlässlichkeit der Meldungen und der Anzahl an positiven Datenpunkten abgewägt werden.
Da es jedoch wichtiger ist, alle künftigen Radarkontrollen vorherzusagen als alle Rasterzellen korrekt vorherzusagen, in denen es keine Radarkontrolle gibt, sollte der Grenzwert relativ gering gewählt werden.
Im Bezug auf die Verteilung der Standdauer in \autoref{fig:StanddauerVerteilung} scheint eine halbe Stunde ein guter Grenzwert zu sein.
Damit wird ein großteil der Meldungen eingeschlossen, wobei die wenig verlässlichen Meldungen unter einer halben Stunde vernachlässigt werden.
Die Klassenzuordnung je nach Überschreitung des Grenzwertes kann mit numpy einfach implementiert werden als \emph{dataset = np.where(dataset > 0.5, 1, 0)}.

Als Nächstes muss der Datensatz in Sequenzen von je 17 Frames aufgeteilt werden, von denen die ersten 16 als Eingabesequenz dienen und das letzte als Zielframe.
Da die Vorhersage von beliebigen Wochentagen möglich sein soll und möglichst viele Trainingsdaten benötigt werden, sollten auch die Sequenzen ausgenutzt werden, die sich überschneiden.
Somit soll beispielsweise aus dem Array $[1,~2,~3,~4]$ bei einer Sequenzlänge von $2$ nicht nur das Array $[[1,~2], [3,~4]]$ entstehen, sondern das Array $[[1,~2],~[2,~3],~[3,~4]]$.
Wird der Datensatz auf diese Weise in Sequenzen unterteilt, entsteht ein Array der Form $(2636,~17,~50,~50)$.
Das Array enthält nun also 2636 Sequenzen aus je 17 Frames mit einer Größe von 50x50 Pixel.
Als Nächstes müssen die Sequenzen in zufällige Trainings- und Validierungsdaten unterteilt werden.
Dazu werden die Sequenzen zufällig gemischt und anschließend werden 90\,\% der Frames dem Trainingsdatensatz zugewiesen und 10\,\% den Validierungsdatensatz.
Die Sequenzen müssen im letzten Schritt noch je in die Ein- und Zielausgabe des \acrshortpl{nn} unterteilt werden.
Dies wird in \autoref{lst:CreateXYFrames} von der Funktion \emph{create\_xy\_frames()} implementiert.

\begin{code}
\begin{minted}[
    linenos,
    numbersep=10pt,
    gobble=0,
    frame=lines,
    framesep=2mm,
    escapeinside=!!]{python}
def create_xy_frames(data):
    x = data[:, 0:-2, :, :] !\label{x_frames}!
    y = data[:, -1, :, :] !\label{y_frames}!
    y = np.expand_dims(y, axis=1) !\label{y_frames_dim}!
    return x, y

x_train, y_train = create_xy_frames(train_dataset)
x_val, y_val = create_xy_frames(val_dataset)
\end{minted}
\captionof{listing}{Implementierung der Funktion \emph{create\_xy\_frames()} zum Unterteilen der Sequenzen in Ein- und Zielausgabe}
\label{lst:CreateXYFrames}
\end{code}

In Zeile \ref{x_frames} wird zunächst die Eingabesequenz extrahiert.
Diese besteht aus dem ersten bis vorletzten Frame, somit hat der Eingabedatensatz \emph{x\_train} schließlich die Form $(2372,~16,~50,~50)$.
Die Zielausgabe hingegen wird in Zeile \ref{y_frames} erstellt besteht nur aus dem letzten Frame jeder Sequenz.
Durch die Auswahl nur eines Elements einer Dimension geht diese Dimension jedoch komplett verloren.
Da das \acrshort{nn} die Dimension jedoch erwartet, muss sie in Zeile \ref{y_frames_dim} wieder hinzugefügt werden, sie hat dann die Länge $1$.
Damit hat die Zielausgabe \emph{y\_train} die Form $(2372,~1,~50,~50)$.
Nun sind die Trainingsdaten bereit und das Modell kann damit trainiert werden.
