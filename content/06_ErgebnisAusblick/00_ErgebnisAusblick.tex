\section{Ergebnis}
\label{sec:Ergebnis}
Im ersten Schritt wurden die Grundlagen des Deep Learnings und einiger wichtiger Modelle erarbeitet.
Dies stellte kein großes Problem dar, da dieses Thema in den letzten Jahren sehr populär geworden ist.
Daher existiert eine Vielzahl an Ressourcen, die dieses komplexe Thema sehr anschaulich erklären.
Dieses Grundwissen wurde anschließend verwendet, um räumlich-zeitliche Vorhersagen zu definieren und geeignete Modelle zu finden.
Hierbei ist anzumerken, dass einige elementare Unterschiede von verschiedenen Anwendungsfällen räumlich-zeitlicher Vorhersagen herausgearbeitet wurden, anhand derer die Anwendungsfälle Eingeordnet werden können.
Dieses Vorgehen konnte nicht in existierender Literatur gefunden werden, daher ist davon auszugehen, dass eine Einordnung anhand der Beschaffenheit der Datensätze in dieser Arbeit erstmals vorgenommen wurde.
Mit ST-ResNet und ConvLSTM konnten zwei mögliche Modelle für räumlich-zeitliche Vorhersagen gefunden werden.
Aufgrund von praktischen Vorteilen wurde ConvLSTM als das geeignetere Modell ausgewählt.

Durch die Analyse des Datensatzes konnten einige Inkonsistenzen festgestellt und erfolgreich behoben werden.
Um eine geeignete Implementierung der Rasterisierung zu finden wurde einiges an Zeit benötigt, jedoch ist die letztendlich gefundene Lösung sehr flexibel.
Für die Implementierung eines \acrshortpl{nn} für den vorliegenden Anwendungsfall konnte die Architektur aus \cite{CrimeConvLSTM} übernommen werden, inkl. der Idee für eine gewichtete Verlustfunktion.
Dadurch konnte hier etwas Zeit eingespart werden.
Nach dem Training des Modells wurde erörtert, wie die rohen Ausgabewerte des Modells nachbearbeitet werden müssen.
Dafür wurde die Auswirkung der Perzentilgrenze untersucht.
Mit der Einteilung in vier Gefahrenstufen wurde eine Möglichkeit gefunden, möglichst einfach verständliche Vorhersagen zu erzeugen.

Zuletzt wurde die Performance des Modells evaluiert.
Dieser Schritt ist sehr wichtig, da er die Sinnhaftigkeit einer Anwendung von Deep Learning auf die vorliegende Problemstellung bestätigen oder wiederlegen soll.
Durch zufälliges Vertauschen der Zielframes konnte bestätigt werden, dass die Vorhersagen tatsächlich von den konkreten Eingabeframes abhängen.
Es wurde mit bestem Wissen und Gewissen dafür gesorgt, dass das Modell die wahren Zielframes zuvor noch nie gesehen hat.
Daher lässt sich schlussfolgen, dass sich die anfängliche Vermutung bestätigt, dass es gewisse Muster in den Daten gibt, anhand derer sich die Standorte von mobilen Radarkontrollen vorhersagen lassen.
Es besteht also teilweise ein direkter Zusammenhang zwischen den Standorten der letzten 16 Tage und denen des Folgetages.
Des weiteren lässt sich aus diesem Ergebnis ableiten, dass Deep Learning und insbesondere die ConvLSTM-Architektur auf die vorliegende Problemstellung anwendbar ist, was nicht von Anfang an klar war.
Mit dem gewählten Vorgehen ist die Vorhersage der Gefahr für mobile Radarkontrollen somit grundsätzlich möglich.
Allerdings ist die Vorhersage fehlerbehaftet, da es sowohl einige Falsch-Negative als auch Falsch-Positive gibt.
Dies war jedoch von Anfang an zu erwarten, da schon der Datensatz nicht perfekt ist.
Außerdem kann nicht davon ausgegangen werden, dass ein komplett kausaler Zusammenhang zwischen den Standorten der letzten 16 Tage und denen des Folgetages besteht.
Daher werden die Vorhersagen immer fehlerbehaftet sein.


\section{Ausblick}
\label{sec:Ausblick}
Es gibt einige Ansätze, wie die Genauigkeit der Vorhersagen auch unter den oben erwähnten Umständen weiter erhöht werden könnte.
Zunächst kann man bei einer räumlichen Betrachtung des Datensatzes schnell erkennen, dass es für die Standorte von mobilen Radarkontrollen Hotspots gibt, die über die Zeit konstant bleiben, wie z.\,B. an bestimmten Kreuzungen.
Daher würde es zum vorliegenden, räumlich kontinuierlichen Datensatz besser passen, diesen als Graph zu betrachten, bei dem Hotspots Knoten und das Straßennetz Kanten entsprechen.
Für graphbasierte Daten gibt es einige vielversprechende Modellarchitekturen, über die in \cite{DLTraff} ein Überblick gegeben wird.
Beispiele hierfür sind die Modelle STGCN oder DCRNN.
Auch der rasterbasierte Ansatz kann noch weiter verfeinert werden.
In \cite{HeteroConvLSTM}, werden beispielsweise im Rahmen der Vorhersage von Verkehrsunfällen einige vielversprechende Vorgehensweisen vorgestellt, die über das Vorgehen der vorliegenden Arbeit deutlich hinausgehen.
Zunächst versuchen die Autoren, Unterschiede zwischen Stadt und Land möglichst gut zu berücksichtigen.
Um ihnen gerecht zu werden, verwenden sie einen Sliding-Window Ansatz, bei dem ein 32x32 Pixel großes Fenster über verschiedene Bereiche der Eingabe gelegt wird.
Somit überwiegt bei jeder einzelnen Position nicht immer der Verlustwert der städtischen Gebiete, was sonst der Fall wäre.
Der zweite große Unterschied zur in der vorliegenden Arbeit verwendeten Implementierung ist die Einbindung von deutlich mehr Merkmalen.
Zusätzlich zu den Verkehrsunfällen der letzten Tage werden noch ca. 30 weitere Merkmalskarten pro Tag verwendet.
Diese enthalten u.\,A. ob sich in einer Zelle überhaupt eine Straße befindet, die Anzahl der Verkehrsknoten, das Verkehrsaufkommen, Wetterdaten, ein RGB-Satellitenbild und den Zustand der Straßen.
Solche zusätzlichen Merkmalskarten könnten auch für die vorliegende Problemstellung verwendet werden.
Außerdem wird die Ausgabe des Modells am Ende mit einer Matrix multipliziert, die alle Rasterzellen auf null setzt, in denen sich keine Straße befindet.

Bei Betrachtung aller dieser Optimierungen wird klar, dass mit der vorliegenden Arbeit nur eine Grundlage geschaffen wurde.
Daher ist es bemerkenswert, dass schon die vorliegende Implementierung einigermaßen gute Vorhersagen erzeugen kann.
Es bleibt interessant, wie sehr die Vorhersagen durch zusätzliche Optimierungen noch verbessert werden könnten.
